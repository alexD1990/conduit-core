# GENERATED BY: conduit template s3_to_bigquery
# DESCRIPTION: Load S3 objects into BigQuery
# LAST UPDATED: 2025-01-18

version: "1.0"
project:
  name: "s3_to_bigquery_pipeline"
  environment: "production"

sources:
  - name: "s3_source"
    type: "s3"
    bucket: "my-data-bucket"          # UPDATE THIS
    path: "path/to/file.csv"          # Object key in bucket

destinations:
  - name: "bigquery_dest"
    type: "bigquery"
    project: "${GCP_PROJECT_ID}"
    dataset: "analytics"              # UPDATE THIS
    table: "target_table"             # UPDATE THIS
    credentials_path: "${GOOGLE_APPLICATION_CREDENTIALS}"

resources:
  - name: "s3_to_bigquery"
    source: "s3_source"
    destination: "bigquery_dest"
    query: ""
    mode: "append"

# HOW TO USE:
# 1. Update bucket, object key, and table fields
# 2. Set AWS and GCP credentials (env vars)
# 3. Save as `pipeline.yml`
# 4. Run: `conduit run pipeline.yml`
