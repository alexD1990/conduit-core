# GENERATED BY: conduit template s3_to_bigquery
# DESCRIPTION: Load S3 objects into BigQuery
# LAST UPDATED: 2025-01-15

version: "1.0"
project:
  name: "s3_to_bigquery_pipeline"           # UPDATE THIS
  environment: "production"

sources:
  - type: "s3"
    name: "s3_source"
    config:
      bucket: "YOUR_BUCKET"                 # UPDATE THIS
      prefix: "path/to/files/"              # UPDATE THIS
      aws_access_key_id: "${AWS_ACCESS_KEY_ID}"
      aws_secret_access_key: "${AWS_SECRET_ACCESS_KEY}"

destinations:
  - type: "bigquery"
    name: "bq_dw"
    config:
      project: "${BQ_PROJECT_ID}"
      dataset: "TARGET_DATASET"             # UPDATE THIS
      table: "TARGET_TABLE"                 # UPDATE THIS
      credentials_json: "${BQ_SERVICE_ACCOUNT_JSON}"

pipelines:
  - name: "s3_to_bigquery_pipeline"
    source: "s3_source"
    destination: "bq_dw"
    mode: "append"

# HOW TO USE:
# 1. Update all fields.
# 2. Set required environment variables for credentials.
# 3. Save as `pipeline.yml`.
# 4. Run: `conduit run pipeline.yml`.
