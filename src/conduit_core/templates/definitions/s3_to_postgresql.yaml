# GENERATED BY: conduit template s3_to_postgresql
# DESCRIPTION: Ingest S3 files into PostgreSQL
# LAST UPDATED: 2025-01-18

version: "1.0"
project:
  name: "s3_to_postgresql_pipeline"
  environment: "production"

sources:
  - name: "s3_source"
    type: "s3"
    bucket: "my-data-bucket"          # UPDATE THIS
    path: "path/to/file.csv"          # Object key in bucket

destinations:
  - name: "pg_dest"
    type: "postgresql"
    host: "${PG_HOST}"
    port: 5432
    database: "${PG_DATABASE}"
    user: "${PG_USER}"
    password: "${PG_PASSWORD}"
    db_schema: "public"
    table: "target_table"             # UPDATE THIS

resources:
  - name: "s3_to_postgresql"
    source: "s3_source"
    destination: "pg_dest"
    query: ""
    mode: "append"

# HOW TO USE:
# 1. Update bucket, object key, and table fields
# 2. Set AWS and PostgreSQL credentials via environment
# 3. Save as `pipeline.yml`
# 4. Run: `conduit run pipeline.yml`
