Metadata-Version: 2.4
Name: conduit-core
Version: 1.0.0
Summary: Declarative, Reliable, Testable Data Ingestion for the Modern Data Stack
Author-email: alex <alex12060309@gmail.com>
Requires-Python: >=3.12
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: typer[all]>=0.19.2
Requires-Dist: pyyaml>=6.0.3
Requires-Dist: pydantic>=2.11.9
Requires-Dist: python-dotenv>=1.1.1
Requires-Dist: pandas>=2.0.0
Requires-Dist: pyarrow>=14.0.0
Requires-Dist: boto3>=1.35.0
Requires-Dist: psycopg2-binary>=2.9.0
Requires-Dist: snowflake-connector-python>=3.0.0
Provides-Extra: dev
Requires-Dist: pytest>=8.4.2; extra == "dev"
Requires-Dist: moto[s3]>=5.0.0; extra == "dev"

# Conduit Core

**Declarative. Reliable. Testable.**  
The **dbt of data ingestion** — a bulletproof open-source CLI for declarative, reliable, and testable data movement.

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)]()
[![Tests Passing](https://img.shields.io/badge/tests-173%20passing-brightgreen)]()
[![License](https://img.shields.io/badge/license-MIT-blue.svg)]()

Conduit Core is an open-source, production-grade data ingestion engine for the modern data stack.  
It focuses on **Extract & Load**, providing atomic operations, schema validation, and data quality enforcement — all from simple YAML.

---

## Vision

Conduit Core moves any data between any sources without breaking. Our philosophy:

- ✅ **It just works** - Auto-detects formats, handles edge cases gracefully
- ✅ **Never crashes** - Bulletproof error handling with retries and recovery
- ✅ **Always transparent** - Real-time progress, detailed logs, dry-run mode
- ✅ **Declarative** - Simple YAML configuration, version-controlled
- ✅ **Production-ready** - Atomic state, checkpoints, resume capability

---

## Core Features (v1.0)

* **Schema Validation:**  
  Pre-flight checks for schema compatibility between source and destination.

* **Schema Evolution:**  
  Automatically detect and handle schema drift (added/removed/changed columns).

* **Data Quality Framework:**  
  Define column-level validation rules (`not_null`, `regex`, `in_list`, `unique`, etc.) in YAML.

* **Atomic Operations:**  
  File writes use temp files → rename; database writes are transactional.

* **Checkpoint & Resume:**  
  Automatically resume long-running jobs from the last successful batch.

* **Incremental Loading:**  
  Process only new records using `incremental_column` with automatic state tracking.

* **CLI Suite:**  
  - `conduit run` — execute pipelines  
  - `conduit validate` — check configuration and schema  
  - `conduit schema` — infer or compare schemas  
  - `conduit manifest` — view detailed run history

## Quick Start

### Installation

Clone the repository
```git clone https://github.com/alexD90/conduit-core.git```

```cd conduit-core```

Install with pip
```pip install -e```

### Verify installation
```conduit --help```

---

# Your First Pipeline
### 1. Create ```ingest.yml```:
```yaml
sources:
  - name: my_csv_file
    type: csv
    path: "./data/users.csv"

destinations:
  - name: output_json
    type: json
    path: "./output/users.json"

resources:
  - name: csv_to_json
    source: my_csv_file
    destination: output_json
    query: "n/a"
```


### 2. Create your source file (data/users.csv):
```yaml
id,name,email
1,Alice,alice@example.com
2,Bob,bob@example.com
```

### 3. Run the pipeline:
```yaml
conduit run

Output:

 Starting Conduit Core run...
 csv_to_json ━━━━━━━━━━━━━━━━━━━━ 100% • 0:00:00 • 1234.5 rows/sec

 Summary for csv_to_json:
 • Rows processed: 2
 • Time elapsed: 0.01s
 • Throughput: 200.0 rows/sec

 Conduit Core run complete!
```

 # Core Concepts

## Sources
Sources define where data comes from:

### sources:
```yaml
  # Local CSV file (auto-detects delimiter and encoding)
  - name: local_csv
    type: csv
    path: "./data/input.csv"
  
  # JSON file
  - name: local_json
    type: json
    path: "./data/input.json"
  
  # Parquet file
  - name: local_parquet
    type: parquet
    path: "./data/input.parquet"
  
```

## Destinations
Destinations define where data goes:

### destinations:
```yaml
  # Local CSV output
  - name: csv_output
    type: csv
    path: "./output/result.csv"
  
  # JSON output
  - name: json_output
    type: json
    path: "./output/result.json"
  
```
## Resources
Resources connect sources to destinations:

### resources:
```yaml
  # Simple transfer
  - name: simple_transfer
    source: local_csv
    destination: json_output
    query: "n/a"
  
    # With SQL query
  - name: filtered_data
    source: postgres_source
    destination: csv_output
    query: "SELECT * FROM users WHERE active = true"
  
  # Incremental loading
  - name: new_orders_only
    source: postgres_source
    destination: json_output
    incremental_column: order_id
    query: "SELECT * FROM orders ORDER BY order_id"
```
## CLI Commands
```conduit run```
Run your data pipeline:
```yaml
# Basic run
conduit run

# Specify config file
conduit run --file my_config.yml

# Dry run (preview without writing)
conduit run --dry-run

# Verbose logging
conduit run --verbose

# Automatic resume
# If a checkpoint exists from a failed run, Conduit Core resumes automatically.
```
## Test all connections before running:

```conduit test```
```yaml
# Output:
 Testing connections...
 
# Sources:
 local_csv (csv): Connected
 postgres_source (postgres): Connected
 
# Destinations:
#json_output (json): Connected
```
## Validate your configuration file

```conduit validate```
```yaml
conduit validate

# Output:
  Validating configuration file: ingest.yml
  Configuration is valid!
```

## Configuration & Secrets

### Environment Variables (```.env```)
### Store sensitive credentials in a ```.env``` file:
```yaml
# AWS (for future S3 connector)
AWS_ACCESS_KEY_ID=your-key
AWS_SECRET_ACCESS_KEY=your-secret
```

### Important: Add ```.env``` to your ```.gitignore```!


## Supported Connectors (v1.0)

| Category     | Sources (Read)              | Destinations (Write)            |
|---------------|----------------------------|---------------------------------|
| **Files**     | CSV, JSON, Parquet, S3     | CSV, JSON, Parquet, S3          |
| **Databases** | PostgreSQL                 | PostgreSQL, Snowflake, BigQuery |


# Advanced Features

## Incremental Loading

Only process new data using an `incremental_column`.  
Conduit Core automatically appends a filter condition (`WHERE <column> > <stored state>`) — no placeholders required.

```yaml
resources:
  - name: new_users
    source: database
    destination: data_lake
    incremental_column: user_id
    query: "SELECT * FROM users ORDER BY user_id"
```

### State is automatically tracked in ```.conduit_state.json```:
```yaml
{
    "new_users": 12345
}
```
On the next run, only records with ```user_id > 12345``` are fetched automatically

## Resume from Failures

### If a pipeline crashes mid-run, resume from the exact point:
```yaml
# Pipeline crashes at row 15,000 out of 100,000
conduit run

# Resume automatically after failure
# Conduit Core detects an existing checkpoint and resumes automatically.
# Skipped 15,000 already-processed rows
# Processing continues from row 15,001
```

## Auto-Detection

### Conduit automatically detects:

- CSV delimiters: ```,``` ```;``` ```\t``` ```|```
- File encodings: UTF-8, Latin-1, Windows-1252
- Data types: integers, floats, booleans, dates, datetimes
- NULL values: ```NULL```, ```None```, ```N/A```,empty strings

## Schema Inference

### Generate SQL tables automatically:
```yaml
from conduit_core.schema import SchemaInferrer, TableAutoCreator

# Infer schema from data
records = [{"id": 1, "name": "Alice", "age": 30}]
schema = SchemaInferrer.infer_schema(records)

# Generate CREATE TABLE statement
sql = TableAutoCreator.generate_create_table_sql(
    "users", 
    schema, 
    dialect="postgresql"
)
print(sql)
 CREATE TABLE IF NOT EXISTS users (
   "id" INTEGER NOT NULL,
   "name" TEXT NOT NULL,
   "age" INTEGER NOT NULL
 );
```

 ## Testing

Conduit Core includes 173 passing tests covering:
- CLI commands
- Engine and state management
- Schema inference and validation
- Data quality checks
- Connectors (CSV, JSON, Parquet, PostgreSQL, S3)

 ### Run the comprehensive test suite:
Run tests with:
```bash
# Run all tests
pytest -v

# Run specific test file
pytest tests/test_config.py -v

# Run with coverage
pytest --cov=conduit_core --cov-report=html

# Run integration tests only
pytest tests/integration/ -v
```

# Architecture

## Project Structure
```yaml
conduit-core/
├── src/conduit_core/
│   ├── batch.py 
│   ├── checkpoint.py       # Resume capability           
│   ├── cli.py              # CLI commands
│   ├── config.py           # Configuration loading
│   ├── engine.py           # Pipeline execution
│   ├── errors.py           # Custom exceptions
│   ├── logging_utils.py    
│   ├── manifest.py   
│   ├── quality.py 
│   ├── schema_evolution.py
│   ├── schema_store.py
│   ├── schema_validator.py
│   ├── schema.py           # Schema inference
│   ├── state.py            # State management
│   ├── types.py            # Type conversion
│   ├── validators.py       # Data validation         
│   └── connectors/
│       ├── base.py         # Base classes
│       ├── registry.py     # Auto-discovery
|       |── bigquery.py     # BigQuery connector
│       ├── csv.py          # CSV connector
│       ├── json.py         # JSON connector
│       ├── parquet.py      # parquet connector
│       ├── s3.py           # S3 connector
│       ├── snowflake.py    # SnowFlake connector
├── tests/
│   ├── test_*.py           # Unit tests
│   ├── integration/        # Integration tests
│   └── fixtures/           # Test data
├── data/                   # Input data (gitignored)
├── output/                 # Output data (gitignored)
├── ingest.yml             # Pipeline configuration
├── .env                   # Secrets (gitignored)
└── README.md
```

## Core Components

1. Config Layer - YAML parsing with Pydantic validation
2. Connector Registry - Auto-discovers all connectors
3. Engine - Orchestrates data flow with progress tracking
4. State Manager - Atomic state persistence with backups
5. Checkpoint System - Periodic progress saves for resume
6. Validators - Data sanitization and validation
7. Type System - Robust type conversion

---
# Troubleshooting

### Common Issues

Problem: ```conduit: command not found```
Solution:
```yaml
### Make sure you installed in editable mode
pip install -e .

### Or run directly
python -m conduit_core.cli run
```

Problem: CSV delimiter not detected correctly
Solution:
```yaml
# Use verbose mode to see detection
conduit run --verbose

# Manually specify in config (future feature)
```

Problem: Database connection fails
Solution:
```yaml
# Test connection first
conduit test

# Check your .env file
cat .env

Problem: Special characters garbled
Solution:
```yaml
# Conduit auto-detects encoding, but you can verify:
conduit run --verbose

# Check source file encoding:
file -i your_file.csv
```

# Contributing

Contributions are welcome! Areas we'd love help with:

- New Connectors (MySQL, MongoDB, REST APIs, etc.)
 - More Tests (especially edge cases)
 - Documentation (examples, tutorials)
 - Bug Fixes
 - Feature Requests

 # License

 MIT License - see LICENSE file for details.

 # Acknowledgments

 Built with:

- Typer - CLI framework
- Rich - Beautiful terminal output
- Pydantic - Data validation
- Pandas - Data manipulation
- PyArrow - Parquet support
